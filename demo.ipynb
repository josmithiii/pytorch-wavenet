{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n",
      "CUDA available: False\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from model_logging import TensorboardLogger\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Debug printing function\n",
    "debug_enabled = False\n",
    "def debug_print(msg):\n",
    "    if debug_enabled:\n",
    "        print(msg)\n",
    "\n",
    "def set_debug(enabled):\n",
    "    global debug_enabled\n",
    "    debug_enabled = enabled\n",
    "\n",
    "# Set debug printing off by default\n",
    "set_debug(False)\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "# Check device availability\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory dataset for better performance\n",
    "class MemoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.length = len(dataset)\n",
    "        self.train = dataset.train if hasattr(dataset, 'train') else True\n",
    "\n",
    "        # Print sample item shape during initialization\n",
    "        sample_item = self.dataset[0]\n",
    "        print(\"\\nDataset sample info:\")\n",
    "        print(f\"Sample item type: {type(sample_item)}\")\n",
    "        if isinstance(sample_item, tuple):\n",
    "            print(f\"Sample x shape: {sample_item[0].shape}\")\n",
    "            print(f\"Sample target shape: {sample_item[1].shape}\")\n",
    "        else:\n",
    "            print(f\"Sample shape: {sample_item.shape}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # Print shape occasionally for debugging\n",
    "        if idx % 1000 == 0 and debug_enabled:\n",
    "            if isinstance(item, tuple):\n",
    "                print(f\"\\nBatch {idx} shapes:\")\n",
    "                print(f\"x: {item[0].shape}\")\n",
    "                print(f\"target: {item[1].shape}\")\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WaveNetModel...\n",
      "Parameters: layers=6, blocks=4, dilation_channels=16, residual_channels=16, skip_channels=32, classes=256, output_length=8\n",
      "Creating start convolution...\n",
      "Creating dilated convolution 0...\n",
      "Creating dilated convolution 1...\n",
      "Creating dilated convolution 2...\n",
      "Creating dilated convolution 3...\n",
      "Creating dilated convolution 4...\n",
      "Creating dilated convolution 5...\n",
      "Creating dilated convolution 0...\n",
      "Creating dilated convolution 1...\n",
      "Creating dilated convolution 2...\n",
      "Creating dilated convolution 3...\n",
      "Creating dilated convolution 4...\n",
      "Creating dilated convolution 5...\n",
      "Creating dilated convolution 0...\n",
      "Creating dilated convolution 1...\n",
      "Creating dilated convolution 2...\n",
      "Creating dilated convolution 3...\n",
      "Creating dilated convolution 4...\n",
      "Creating dilated convolution 5...\n",
      "Creating dilated convolution 0...\n",
      "Creating dilated convolution 1...\n",
      "Creating dilated convolution 2...\n",
      "Creating dilated convolution 3...\n",
      "Creating dilated convolution 4...\n",
      "Creating dilated convolution 5...\n",
      "WaveNetModel initialization complete\n",
      "Looking for models in snapshots\n",
      "Found files: ['snapshots/checkpoint_epoch_0.pt', 'snapshots/best_model.pt', 'snapshots/chaconne_model_2017-12-28_16-44-12']\n",
      "Loading original chaconne model: snapshots/chaconne_model_2017-12-28_16-44-12\n",
      "Loading model from snapshots/chaconne_model_2017-12-28_16-44-12...\n",
      "Using original model format\n",
      "Model device after transfer: mps:0\n",
      "Start conv weight device: mps:0\n",
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1834592\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=6,\n",
    "                     blocks=4,\n",
    "                     dilation_channels=16,\n",
    "                     residual_channels=16,\n",
    "                     skip_channels=32,\n",
    "                     output_length=8,\n",
    "                     bias=False)\n",
    "\n",
    "# Load latest model if available\n",
    "model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "\n",
    "# Ensure model is on correct device\n",
    "model = model.to(device)\n",
    "print(f\"Model device after transfer: {next(model.parameters()).device}\")\n",
    "print(f\"Start conv weight device: {model.start_conv.weight.device}\")\n",
    "\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (9594672,), dtype: uint8\n",
      "Converted data shape: (9594672,), dtype: int64\n",
      "Loading dataset file: train_samples/bach_chaconne/dataset.npz\n",
      "Available keys in dataset: ['arr_0']\n",
      "the dataset has 479579 items\n",
      "Dataset type: <class 'numpy.ndarray'>\n",
      "Dataset shape: (9594672,)\n",
      "Dataset dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "data = WavenetDataset(dataset_file='train_samples/bach_chaconne/dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples/bach_chaconne',\n",
    "                      test_stride=20)\n",
    "\n",
    "# Load dataset file and print info\n",
    "print(\"Loading dataset file:\", data.dataset_file)\n",
    "with np.load(data.dataset_file) as dataset:\n",
    "    print(\"Available keys in dataset:\", dataset.files)\n",
    "    data.data = dataset['arr_0']\n",
    "\n",
    "print('the dataset has ' + str(len(data)) + ' items')\n",
    "print(f\"Dataset type: {type(data.data)}\")\n",
    "print(f\"Dataset shape: {data.data.shape}\")\n",
    "print(f\"Dataset dtype: {data.data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sample info:\n",
      "Sample item type: <class 'tuple'>\n",
      "Sample x shape: torch.Size([256, 3085])\n",
      "Sample target shape: torch.Size([16])\n",
      "\n",
      "splitting dataset:\n",
      "  total size: 479579\n",
      "  train size: 431621\n",
      "  val size: 47958\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m memory_dataset \u001b[38;5;241m=\u001b[39m MemoryDataset(data)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create trainer with memory_dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mWavenetTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# JOS: was 8\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# JOS: was 32\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_subset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnapshots\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/w/pytorch-wavenet/wavenet_training.py:92\u001b[0m, in \u001b[0;36mWavenetTrainer.__init__\u001b[0;34m(self, model, dataset, batch_size, val_batch_size, val_subset_size, lr, weight_decay, snapshot_interval, snapshot_path, val_interval, gradient_clipping)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  val size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m random_split(\n\u001b[1;32m     84\u001b[0m     dataset,\n\u001b[1;32m     85\u001b[0m     [train_size, val_size],\n\u001b[1;32m     86\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     90\u001b[0m     train_dataset,\n\u001b[1;32m     91\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m---> 92\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[43mtrue\u001b[49m,\n\u001b[1;32m     93\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     94\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader \u001b[38;5;241m=\u001b[39m dataloader(\n\u001b[1;32m     98\u001b[0m     val_dataset,\n\u001b[1;32m     99\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mval_batch_size,\n\u001b[1;32m    100\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mfalse,\n\u001b[1;32m    101\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdataloaders created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "# Create memory dataset for better performance\n",
    "memory_dataset = MemoryDataset(data)\n",
    "\n",
    "# Create trainer with memory_dataset\n",
    "trainer = WavenetTrainer(\n",
    "    model=model,\n",
    "    dataset=memory_dataset,\n",
    "    batch_size=2, # JOS: was 8\n",
    "    val_batch_size=2, # JOS: was 32\n",
    "    val_subset_size=500,\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0,\n",
    "    gradient_clipping=1,\n",
    "    snapshot_interval=1000,\n",
    "    snapshot_path='snapshots',\n",
    "    val_interval=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_audio function\n",
    "def generate_audio(model, length=16000, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate audio samples using the trained model.\n",
    "    Args:\n",
    "        model: Trained WaveNet model\n",
    "        length: Number of samples to generate\n",
    "        temperature: Controls randomness (higher = more random, lower = more deterministic)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Move model to device if not already\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Start with zeros\n",
    "    current_sample = torch.zeros(1, 1, model.receptive_field).to(device)\n",
    "    generated_samples = []\n",
    "\n",
    "    print(f\"\\nGenerating {length} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(length)):\n",
    "            # Get model prediction\n",
    "            output = model(current_sample)\n",
    "\n",
    "            # Apply temperature\n",
    "            if temperature != 1:\n",
    "                output = output / temperature\n",
    "\n",
    "            # Sample from the output distribution\n",
    "            probabilities = F.softmax(output[:, :, -1], dim=1)\n",
    "            next_sample = torch.multinomial(probabilities, 1)\n",
    "\n",
    "            # Append to generated samples\n",
    "            generated_samples.append(next_sample.item())\n",
    "\n",
    "            # Shift input window and add new sample\n",
    "            current_sample = torch.roll(current_sample, -1, dims=2)\n",
    "            current_sample[0, 0, -1] = next_sample\n",
    "\n",
    "    # Convert to numpy array\n",
    "    samples = np.array(generated_samples, dtype=np.int16)\n",
    "\n",
    "    # Scale back to audio range\n",
    "    samples = samples - 2**15\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_and_log_samples function\n",
    "def generate_and_log_samples(step):\n",
    "    sample_length = 4000\n",
    "    gen_model = load_latest_model_from('snapshots')\n",
    "    gen_model = gen_model.to(device)\n",
    "    \n",
    "    print(\"Start generating samples for logging...\")\n",
    "    \n",
    "    # Generate with temperature 0\n",
    "    samples_0 = generate_audio(gen_model, length=sample_length, temperature=0)\n",
    "    \n",
    "    # Generate with temperature 0.5\n",
    "    samples_05 = generate_audio(gen_model, length=sample_length, temperature=0.5)\n",
    "    \n",
    "    # Log samples if TensorFlow is available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf_samples_0 = tf.convert_to_tensor(samples_0, dtype=tf.float32)\n",
    "        logger.audio_summary('temperature 0', tf_samples_0, step, sr=16000)\n",
    "        \n",
    "        tf_samples_05 = tf.convert_to_tensor(samples_05, dtype=tf.float32)\n",
    "        logger.audio_summary('temperature 0.5', tf_samples_05, step, sr=16000)\n",
    "    except ImportError:\n",
    "        print(\"TensorFlow not available, skipping audio logging\")\n",
    "        # Save as WAV files instead\n",
    "        wavfile.write(f'generated_temp0_step{step}.wav', 16000, samples_0)\n",
    "        wavfile.write(f'generated_temp05_step{step}.wav', 16000, samples_05)\n",
    "        \n",
    "    print(\"Audio clips generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up TensorboardLogger\n",
    "try:\n",
    "    logger = TensorboardLogger(log_interval=200,\n",
    "                            validation_interval=200,\n",
    "                            generate_interval=500,\n",
    "                            generate_function=generate_and_log_samples,\n",
    "                            log_dir=\"logs\")\n",
    "    print(\"TensorboardLogger initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing TensorboardLogger: {e}\")\n",
    "    print(\"Continuing without logging...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start training with error handling\n",
    "print('\\nStarting training...')\n",
    "tic = time.time()\n",
    "try:\n",
    "    trainer.train(epochs=2) # JOS: was 20\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    raise\n",
    "toc = time.time()\n",
    "print('Training took {} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set to evaluation mode\n",
    "memory_dataset.train = False\n",
    "trainer.dataloader.dataset.train = False\n",
    "\n",
    "print(\"Dataloader length:\", len(trainer.dataloader))\n",
    "print(\"Test dataset length:\", len(memory_dataset))\n",
    "print(\"Sample length:\", data.item_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate audio samples\n",
    "# Get a starting sample from the dataset\n",
    "start_data = data[100][0]\n",
    "start_data = torch.max(start_data, 0)[1]\n",
    "print(\"Starting data shape:\", start_data.shape)\n",
    "print(\"Starting data sample:\", start_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Progress callback for generation\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "# Make sure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate audio using the fast method\n",
    "try:\n",
    "    # Ensure dilated queues are on the correct device\n",
    "    for q in model.dilated_queues:\n",
    "        q = q.to(device)\n",
    "        \n",
    "    generated1 = model.generate_fast(num_samples=160000, \n",
    "                                    first_samples=start_data,\n",
    "                                    progress_callback=prog_callback,\n",
    "                                    progress_interval=1000,\n",
    "                                    temperature=1.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error during fast generation: {e}\")\n",
    "    print(\"Falling back to standard generation...\")\n",
    "    generated1 = generate_audio(model, length=160000, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Play the generated audio\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated1, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the generated audio\n",
    "output_file = \"generated_audio.wav\"\n",
    "wavfile.write(output_file, 16000, generated1)\n",
    "print(f\"Generated audio saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the generated audio\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(generated1); ax1.set_title('Raw audio signal')\n",
    "ax2.specgram(generated1, Fs=16000); ax2.set_title('Spectrogram');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "env_pytorch_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
